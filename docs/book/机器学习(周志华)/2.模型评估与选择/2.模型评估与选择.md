# 2. 模型评估与选择

## 2.1经验误差与过拟合

`通常将分类错误的样本数占样本总数的比例称为“错误率”`，即`如果在m个样本中有a个样本分类错误`，则`错误率`为`a/m`；相应的，`1-a/m`称为`精度`，即`精度=1-错误率`。

更一般地，我们把学习器的实际预测输出与样本的真实输出之间的差异，称为“误差”，学习器在训练集上的误差称为`“训练误差”或“经验误差”`，在新样本上的误差称为`“泛化误差”`。

我们实际希望的是“泛化误差”更小的学习器，为了达到这个目的，应该从训练样本中尽可能学出适用于所有潜在样本的“普通规律”。当学习器把训练样本学得“太好”的时候，很可能已经`把训练样本自身的一些特点当做了潜在样本都会具有的一般性质，这样就会导致泛化能力下降`。这种现象在机器学习中称作`“过拟合”`。与`“过拟合”`相对的是`“欠拟合”`，这是指`对训练样本的一般性质未学好`。

![](2.1.png)

过拟合是机器学习面临的关键障碍，而且过拟合是无法避免的，只能缓解，或者说减少其风险。关于这一点，可大致这样理解：

`机器学习面临的问题通常是NP难甚至更难，而有效的学习算法必然是在多项式时间内运行完成，若可彻底避免过拟合，则通过经验误差最小化就能获最优解，这就意味着我们构造性地证明了“P=NP”；因此，只要相信“P≠NP”，过拟合就不可避面。`

如果想要理解上面这句话可以阅读以下[如下链接](https://zhuanlan.zhihu.com/p/143003261)中的文章。

现实生活中，有多种学习算法可供选择，甚至对于同一个算法，如果参数不同，产生的模型会出现很大差异。因此就存在了“模型选择问题”。

理想的解决方案是对候选模型的泛化误差进行评估，然后选择泛化误差最小的模型，但上面已经提过，我们无法直接获得泛化误差，而训练误差又由于过拟合现象的存在而不适合作为标准，因此需要一些特殊的评估方法。

## 2.2 评估方法

通常，我们可以通过实验测试来对学习器的泛化误差进行评估并做出选择。为此，需要使用一个“测试集”来测试学习期对新样本的判别能力，然后以“测试集”上的“测试误差”作为泛化误差的近似。通常我们假设测试样本也是从样本真实分布中独立同分布采样而得。但需注意的是，测试集应该尽可能与训练集互斥，即测试样本尽量不在训练集中出现、未在训练过程中使用过。

但通常我们只能获取一个m个样例的数据集，如何将其分为训练集、测试集呢？

### 2.2.1 留出法

“留出法”（holdout）`直接将数据集D划分为两个互斥的集合，其中一个集合作为训练集S，另一个作为测试集T，即D=S∪T，S∩T=φ。在S上训练出模型后，用T来评估其测试误差，作为对泛化误差的估计`。

`训练/测试集的划分要尽可能保证数据分布的一致性，避免因为数据划分过程引入额外的偏差而对最终结果产生影响`。如果从采样（sampling）的角度来看待数据集的划分过程，则保留类别比例的采样方式通常称为“分层采样”（stratifiedsampling）。`分类任务中至少要保持样本的类别比例相似`。

另一个需要注意的问题是，`即便在给定训练/测试集的样本比例后，仍存在多种方式对初始数据进行分割。这些不同的划分将导致不同的训练/测试集，相应的，模型评估的结果也会有差别`。因此，单次使用留出法得到的估计结果往往不够稳定可靠，在使用留出法时，`一般要采用若干次随机划分、重复进行实验评估后取平均值作为留出法的评估结果`。

此外，我们希望评估的是用D训练出的模型的性能，但留出法需划分训练/测试集，这就会导致一个窘境：`若令训练集S包含绝大多数样本，则训练出的模型可能更接近于用D训练出的模型，但由于T比较小，评估结果可能不够稳定准确；若令测试集T多包含一些样本，则训练集S与D差别更大了，被评估的模型与用D训练出的模型相比可能有较大差别，从而降低了评估结果的保真性（fidelity）`。这个问题没有完美的解决方案，常见做法是将大约2/3～4/5的样本用于训练，剩余样本用于测试。

### 2.2.2 交叉验证法

`“交叉验证法”`先将数据集D划分为k个大小相似的互斥子集，即D=D1∪D2∪D3∪...∪Dk，Di ∩ Dj = φ(i≠j)。